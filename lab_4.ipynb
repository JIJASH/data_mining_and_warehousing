{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hiVQol2-sXGZTkehujmEdS4bpompwUbM",
      "authorship_tag": "ABX9TyO2YOVkK5FozuICaOScI6l4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JIJASH/data_mining_and_warehousing/blob/main/lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab IV: Clustering and Outlier Detection Algorithms\n",
        "\n",
        "Implementation of clustering and outlier detection algorithms from scratch."
      ],
      "metadata": {
        "id": "_B2fqtlsyhrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RBtFAVcvvKqH"
      },
      "outputs": [],
      "source": [
        "# Import libraries and utility functions\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def euclidean_distance(p1, p2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(p1, p2)))\n",
        "\n",
        "def read_csv(filename):\n",
        "    data = []\n",
        "    with open(filename, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            data.append([float(x) for x in row])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = read_csv('/content/drive/MyDrive/data_mining_and_warehousing/data.csv')      # For algorithms 1-5\n",
        "data2 = read_csv('/content/drive/MyDrive/data_mining_and_warehousing/data2.csv')     # For DBSCAN\n",
        "data3 = read_csv('/content/drive/MyDrive/data_mining_and_warehousing/data3.csv')     # For outlier analysis\n",
        "data4 = read_csv('/content/drive/MyDrive/data_mining_and_warehousing/data4.csv')     # For mini-batch k-means"
      ],
      "metadata": {
        "id": "pTY3saxiykPq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. K-Means Algorithm\n",
        "def kmeans(data, k, max_iterations=100):\n",
        "    n_features = len(data[0])\n",
        "\n",
        "    # Initialize centroids randomly\n",
        "    min_vals = [min(point[i] for point in data) for i in range(n_features)]\n",
        "    max_vals = [max(point[i] for point in data) for i in range(n_features)]\n",
        "    centroids = [[random.uniform(min_vals[i], max_vals[i]) for i in range(n_features)] for _ in range(k)]\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Assign points to closest centroid\n",
        "        clusters = [[] for _ in range(k)]\n",
        "        for point in data:\n",
        "            distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "            closest_centroid = distances.index(min(distances))\n",
        "            clusters[closest_centroid].append(point)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = []\n",
        "        for cluster in clusters:\n",
        "            if cluster:\n",
        "                centroid = [sum(point[i] for point in cluster) / len(cluster) for i in range(n_features)]\n",
        "                new_centroids.append(centroid)\n",
        "            else:\n",
        "                new_centroids.append(centroids[clusters.index(cluster)])\n",
        "\n",
        "        # Check convergence\n",
        "        if all(euclidean_distance(old, new) < 1e-6 for old, new in zip(centroids, new_centroids)):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return centroids, clusters\n",
        "\n",
        "# Test on data.csv\n",
        "centroids, clusters = kmeans(data1, 3)\n",
        "print(\"K-Means centroids:\", centroids)\n",
        "print(\"Cluster sizes:\", [len(cluster) for cluster in clusters])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tcl4jS4zKpX",
        "outputId": "8a7957e2-f09d-4ea3-bb7e-d44b47b88741"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Means centroids: [[1.8251254225611786, 2.291875885218977], [7.0, 1.0], [5.882762642208976, 5.744872196239354]]\n",
            "Cluster sizes: [11, 1, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. K-Means++ Algorithm\n",
        "def kmeans_plus_plus(data, k, max_iterations=100):\n",
        "    n_features = len(data[0])\n",
        "    centroids = [data[random.randint(0, len(data) - 1)]]\n",
        "\n",
        "    # Choose remaining centroids using K-means++ method\n",
        "    for _ in range(1, k):\n",
        "        distances = []\n",
        "        for point in data:\n",
        "            min_dist = min(euclidean_distance(point, centroid) for centroid in centroids)\n",
        "            distances.append(min_dist ** 2)\n",
        "\n",
        "        total_dist = sum(distances)\n",
        "        probabilities = [d / total_dist for d in distances]\n",
        "\n",
        "        r = random.random()\n",
        "        cumulative_prob = 0\n",
        "        for i, prob in enumerate(probabilities):\n",
        "            cumulative_prob += prob\n",
        "            if r <= cumulative_prob:\n",
        "                centroids.append(data[i])\n",
        "                break\n",
        "\n",
        "    # Run standard K-means\n",
        "    for iteration in range(max_iterations):\n",
        "        clusters = [[] for _ in range(k)]\n",
        "        for point in data:\n",
        "            distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "            closest_centroid = distances.index(min(distances))\n",
        "            clusters[closest_centroid].append(point)\n",
        "\n",
        "        new_centroids = []\n",
        "        for cluster in clusters:\n",
        "            if cluster:\n",
        "                centroid = [sum(point[i] for point in cluster) / len(cluster) for i in range(n_features)]\n",
        "                new_centroids.append(centroid)\n",
        "            else:\n",
        "                new_centroids.append(centroids[clusters.index(cluster)])\n",
        "\n",
        "        if all(euclidean_distance(old, new) < 1e-6 for old, new in zip(centroids, new_centroids)):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return centroids, clusters\n",
        "\n",
        "# Test on data.csv\n",
        "centroids_pp, clusters_pp = kmeans_plus_plus(data1, 3)\n",
        "print(\"K-Means++ centroids:\", centroids_pp)\n",
        "print(\"Cluster sizes:\", [len(cluster) for cluster in clusters_pp])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au_PEpUCzQBm",
        "outputId": "c51eb765-29df-4ed6-e776-46d69e79bcbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Means++ centroids: [[1.8251254225611786, 2.291875885218977], [5.882762642208976, 5.744872196239354], [7.0, 1.0]]\n",
            "Cluster sizes: [11, 10, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. K-Medoids Algorithm\n",
        "def k_medoids(data, k, max_iterations=100):\n",
        "    medoids = random.sample(data, k)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        clusters = [[] for _ in range(k)]\n",
        "        for point in data:\n",
        "            distances = [euclidean_distance(point, medoid) for medoid in medoids]\n",
        "            closest_medoid = distances.index(min(distances))\n",
        "            clusters[closest_medoid].append(point)\n",
        "\n",
        "        new_medoids = []\n",
        "        for i, cluster in enumerate(clusters):\n",
        "            if not cluster:\n",
        "                new_medoids.append(medoids[i])\n",
        "                continue\n",
        "\n",
        "            min_total_dist = float('inf')\n",
        "            best_medoid = medoids[i]\n",
        "            for candidate in cluster:\n",
        "                total_dist = sum(euclidean_distance(candidate, point) for point in cluster)\n",
        "                if total_dist < min_total_dist:\n",
        "                    min_total_dist = total_dist\n",
        "                    best_medoid = candidate\n",
        "            new_medoids.append(best_medoid)\n",
        "\n",
        "        if all(old == new for old, new in zip(medoids, new_medoids)):\n",
        "            break\n",
        "        medoids = new_medoids\n",
        "\n",
        "    return medoids, clusters\n",
        "\n",
        "# Test on data.csv\n",
        "medoids, clusters_medoids = k_medoids(data1, 3)\n",
        "print(\"K-Medoids medoids:\", medoids)\n",
        "print(\"Cluster sizes:\", [len(cluster) for cluster in clusters_medoids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHd5O7aBzSUE",
        "outputId": "0ad77fa9-c6dd-4cc7-f9d5-8b2c499d93d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Medoids medoids: [[6.0472697432815465, 5.00267626965058], [1.882923312638332, 1.8829315215254097], [5.1943044958043885, 6.26298861284197]]\n",
            "Cluster sizes: [6, 10, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Agglomerative Hierarchical Clustering\n",
        "def agglomerative_clustering(data, n_clusters):\n",
        "    clusters = [[point] for point in data]\n",
        "\n",
        "    while len(clusters) > n_clusters:\n",
        "        min_distance = float('inf')\n",
        "        merge_i, merge_j = 0, 1\n",
        "\n",
        "        for i in range(len(clusters)):\n",
        "            for j in range(i + 1, len(clusters)):\n",
        "                dist = min(euclidean_distance(p1, p2) for p1 in clusters[i] for p2 in clusters[j])\n",
        "                if dist < min_distance:\n",
        "                    min_distance = dist\n",
        "                    merge_i, merge_j = i, j\n",
        "\n",
        "        merged_cluster = clusters[merge_i] + clusters[merge_j]\n",
        "        new_clusters = [clusters[i] for i in range(len(clusters)) if i != merge_i and i != merge_j]\n",
        "        new_clusters.append(merged_cluster)\n",
        "        clusters = new_clusters\n",
        "\n",
        "    return clusters\n",
        "\n",
        "# Test on data.csv\n",
        "agg_clusters = agglomerative_clustering(data1, 3)\n",
        "print(\"Agglomerative cluster sizes:\", [len(cluster) for cluster in agg_clusters])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq1Yfm-0zU3z",
        "outputId": "66019311-fa78-4413-87a9-e79fc76d707f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agglomerative cluster sizes: [1, 10, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Divisive Clustering\n",
        "def divisive_clustering(data, n_clusters):\n",
        "    clusters = [data]\n",
        "\n",
        "    while len(clusters) < n_clusters:\n",
        "        max_variance = -1\n",
        "        split_index = 0\n",
        "\n",
        "        for i, cluster in enumerate(clusters):\n",
        "            if len(cluster) <= 1:\n",
        "                continue\n",
        "\n",
        "            n_features = len(cluster[0])\n",
        "            variance = 0\n",
        "            for feature in range(n_features):\n",
        "                feature_values = [point[feature] for point in cluster]\n",
        "                mean_val = sum(feature_values) / len(feature_values)\n",
        "                variance += sum((val - mean_val) ** 2 for val in feature_values)\n",
        "\n",
        "            if variance > max_variance:\n",
        "                max_variance = variance\n",
        "                split_index = i\n",
        "\n",
        "        cluster_to_split = clusters[split_index]\n",
        "        if len(cluster_to_split) <= 1:\n",
        "            break\n",
        "\n",
        "        sub_centroids, sub_clusters = kmeans(cluster_to_split, 2, max_iterations=50)\n",
        "        new_clusters = [clusters[i] for i in range(len(clusters)) if i != split_index]\n",
        "        new_clusters.extend(sub_clusters)\n",
        "        clusters = new_clusters\n",
        "\n",
        "    return clusters\n",
        "\n",
        "# Test on data.csv\n",
        "div_clusters = divisive_clustering(data1, 3)\n",
        "print(\"Divisive cluster sizes:\", [len(cluster) for cluster in div_clusters])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCc41m6gzXah",
        "outputId": "05ec5859-fd16-4578-c7ff-4708ed21be83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Divisive cluster sizes: [11, 10, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. DBSCAN Algorithm\n",
        "def dbscan(data, eps, min_pts):\n",
        "    n_points = len(data)\n",
        "    labels = [-1] * n_points  # -1: unclassified, -2: noise\n",
        "    cluster_id = 0\n",
        "\n",
        "    def get_neighbors(point_idx):\n",
        "        neighbors = []\n",
        "        for i in range(n_points):\n",
        "            if euclidean_distance(data[point_idx], data[i]) <= eps:\n",
        "                neighbors.append(i)\n",
        "        return neighbors\n",
        "\n",
        "    def expand_cluster(point_idx, neighbors, cluster_id):\n",
        "        labels[point_idx] = cluster_id\n",
        "        i = 0\n",
        "        while i < len(neighbors):\n",
        "            neighbor_idx = neighbors[i]\n",
        "            if labels[neighbor_idx] == -1:\n",
        "                labels[neighbor_idx] = cluster_id\n",
        "                new_neighbors = get_neighbors(neighbor_idx)\n",
        "                if len(new_neighbors) >= min_pts:\n",
        "                    neighbors.extend(new_neighbors)\n",
        "            elif labels[neighbor_idx] == -2:\n",
        "                labels[neighbor_idx] = cluster_id\n",
        "            i += 1\n",
        "\n",
        "    for point_idx in range(n_points):\n",
        "        if labels[point_idx] != -1:\n",
        "            continue\n",
        "        neighbors = get_neighbors(point_idx)\n",
        "        if len(neighbors) < min_pts:\n",
        "            labels[point_idx] = -2\n",
        "        else:\n",
        "            expand_cluster(point_idx, neighbors, cluster_id)\n",
        "            cluster_id += 1\n",
        "\n",
        "    clusters = defaultdict(list)\n",
        "    noise_points = []\n",
        "    for i, label in enumerate(labels):\n",
        "        if label == -2:\n",
        "            noise_points.append(data[i])\n",
        "        else:\n",
        "            clusters[label].append(data[i])\n",
        "\n",
        "    return dict(clusters), noise_points\n",
        "\n",
        "# Test on data2.csv\n",
        "clusters_dbscan, noise_points = dbscan(data2, 0.5, 3)\n",
        "print(\"DBSCAN clusters:\", len(clusters_dbscan))\n",
        "print(\"Noise points:\", len(noise_points))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQK3G6Cbzacn",
        "outputId": "07f9b53b-ae73-4134-9f96-55ceb1e5f236"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DBSCAN clusters: 1\n",
            "Noise points: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Z-Score Outlier Detection\n",
        "def z_score_outlier_detection(data, threshold=2.0):\n",
        "    values = [point[0] for point in data] if isinstance(data[0], list) else data\n",
        "    n = len(values)\n",
        "\n",
        "    mean = sum(values) / n\n",
        "    variance = sum((x - mean) ** 2 for x in values) / n\n",
        "    std_dev = math.sqrt(variance)\n",
        "\n",
        "    outliers = []\n",
        "    for i, value in enumerate(values):\n",
        "        z_score = (value - mean) / std_dev if std_dev != 0 else 0\n",
        "        if abs(z_score) > threshold:\n",
        "            outliers.append((i, value))\n",
        "\n",
        "    return outliers\n",
        "\n",
        "# Test on data3.csv\n",
        "outliers_zscore = z_score_outlier_detection(data3)\n",
        "print(\"Z-Score outliers:\", len(outliers_zscore))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1I4ia23zcYK",
        "outputId": "2b199c3c-2c86-4fb6-f4cf-dd48b88ddef9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z-Score outliers: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. IQR Outlier Detection\n",
        "def iqr_outlier_detection(data):\n",
        "    values = [point[0] for point in data] if isinstance(data[0], list) else data\n",
        "    sorted_values = sorted(values)\n",
        "    n = len(sorted_values)\n",
        "\n",
        "    def get_quartile(sorted_list, quartile):\n",
        "        index = (len(sorted_list) - 1) * quartile\n",
        "        if index.is_integer():\n",
        "            return sorted_list[int(index)]\n",
        "        else:\n",
        "            lower = sorted_list[int(index)]\n",
        "            upper = sorted_list[int(index) + 1]\n",
        "            return lower + (upper - lower) * (index - int(index))\n",
        "\n",
        "    q1 = get_quartile(sorted_values, 0.25)\n",
        "    q3 = get_quartile(sorted_values, 0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    outliers = []\n",
        "    for i, value in enumerate(values):\n",
        "        if value < lower_bound or value > upper_bound:\n",
        "            outliers.append((i, value))\n",
        "\n",
        "    return outliers\n",
        "\n",
        "# Test on data3.csv\n",
        "outliers_iqr = iqr_outlier_detection(data3)\n",
        "print(\"IQR outliers:\", len(outliers_iqr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh-XDUksze24",
        "outputId": "b55b8616-7d47-41d8-a362-74bbf5dd537b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IQR outliers: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Mini-Batch K-Means Algorithm\n",
        "def mini_batch_kmeans(data, k, batch_size=10, max_iterations=100):\n",
        "    n_features = len(data[0])\n",
        "\n",
        "    min_vals = [min(point[i] for point in data) for i in range(n_features)]\n",
        "    max_vals = [max(point[i] for point in data) for i in range(n_features)]\n",
        "    centroids = [[random.uniform(min_vals[i], max_vals[i]) for i in range(n_features)] for _ in range(k)]\n",
        "    counts = [0] * k\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        batch = random.sample(data, min(batch_size, len(data)))\n",
        "\n",
        "        for point in batch:\n",
        "            distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "            closest_idx = distances.index(min(distances))\n",
        "            counts[closest_idx] += 1\n",
        "            learning_rate = 1.0 / counts[closest_idx]\n",
        "\n",
        "            for i in range(n_features):\n",
        "                centroids[closest_idx][i] = (1 - learning_rate) * centroids[closest_idx][i] + learning_rate * point[i]\n",
        "\n",
        "    clusters = [[] for _ in range(k)]\n",
        "    for point in data:\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        closest_centroid = distances.index(min(distances))\n",
        "        clusters[closest_centroid].append(point)\n",
        "\n",
        "    return centroids, clusters\n",
        "\n",
        "# Test on data4.csv\n",
        "centroids_mb, clusters_mb = mini_batch_kmeans(data4, 4, 20)\n",
        "print(\"Mini-Batch K-Means centroids:\", centroids_mb)\n",
        "print(\"Cluster sizes:\", [len(cluster) for cluster in clusters_mb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdEIfnIBzgnG",
        "outputId": "cdfa5fff-ff33-4efb-ceb5-946c88d8e9fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini-Batch K-Means centroids: [[3.675556769339268, -5.730389523141737], [-6.776849326483863, -6.798302047358834], [4.614522019219613, 2.0748345517529203], [-2.672233536377532, 8.937587930983076]]\n",
            "Cluster sizes: [0, 66, 67, 67]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNUJP5IgzjDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}